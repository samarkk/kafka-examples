# start ksql server
/home/vagrant/confluent/bin/ksql-server-start -daemon /home/vagrant/confluent/etc/ksqldb/ksql-server.properties
# in ksql-server.properties default listen address is 0.0.0.0:8088
# change the port there
# and to start ksql to connect to server on that port do
ksql http://<host>:<port>
# create a topic
kafka-topics --zookeeper master.e4rlearning.com:2181 --create --topic USERS --partitions 1 --replication-factor 1;

# write to the USERS topic from console producer
kafka-console-producer --bootstrap-server master.e4rlearning.com:9092 --topic USERS << EOF
Rajnish, Delhi
Saloni, Kota
Dimple, Delhi
Edward, Mumbai
Kristzian, Bengaluru
Mohit, Chennai
Revathi, Hyderabad
Ananth, Hyderabad
Sudhanshu, Sonepat
Vikrant, Panipat
EOF

# in the shell see messages from the topic
print 'USERS';
# to see users from beginning
print 'USERS' from beginning;
# limit number of records
print 'USERS' from beginning limit 3;
print 'USERS' from beginning interval 2 limit 3;

# create a users stream linked to the users topic
# the value is a string and the delimited format will split the value into fields as per the delimiter
create stream users_stream(name varchar, citycode varchar) 
with (kafka_topic='USERS',value_format='DELIMITED');

# to see the stream contents
select * from users_stream emit changes;

# set offset to earliest
set 'auto.offset.reset'='earliest';

# carry out a grouping
select citycode, count(*) as ccount from users_stream
group by citycode
emit changes;

# delete the stream
drop stream if exists users_stream delete topic;

###############################################
###   JSON and stream                       ###
###############################################
# create topic USERPROFILE
kafka-topics --zookeeper master.e4rlearning.com:2181 --create --topic USERPROFILE --partitions 1 --replication-factor 1;

# write a json record to the topic
kafka-console-producer --topic USERPROFILE --bootstrap-server master.e4rlearning.com:9092 << EOF
 {"userid":1000,"firstname":"santosh","lastname":"rai","citycode":"BL","rating":4.7}
{"userid":1001, "firstname":"Shumon", "lastname":"Saha", "citycode":"DL", "rating":4.8}
EOF

# create stream using JSON
create stream userprofilestream(userid INT, firstname VARCHAR, lastname VARCHAR, citycode VARCHAR, rating DOUBLE) WITH (KAFKA_TOPIC='USERPROFILE',value_format='JSON');


# run a query against the json stream
SELECT CITYCODE, MAX(RATING) AS MXRATING FROM USERPROFILESTREAM 
GROUP BY CITYCODE EMIT CHANGES;

##############################
ksql-datagen
##############################
# just issue ksql-datagen
# needs schema registry, message rate, key, value format, delimiter if using delimited format, iterations - if not specified ksql datagen will produce indefinitely , needs an avro schema file

ksql-datagen schema=/home/vagrant/c/kafka-examples/ksql/schemas/userprofile.avro format=json topic=USERPROFILE key=userid  iterations=1000 msgRate=1

 select rowtime, firstname, lastname, citycode, rating from userprofilestream emit changes limit 5;

# using a function with a kstream
 SELECT rowtime, TIMESTAMPTOSTRING(ROWTIME, 'dd-mm-yyyy hh:mm:ss') as ctime, firstname, lastname, citycode, rating from userprofilestream emit changes limit 5;

 SELECT TIMESTAMPTOSTRING(ROWTIME, 'dd/MMM hh:mm:ss') as createtime, FIRSTNAME + ' ' + ucase(LASTNAME) as fullname from userprofilestream emit changes limit 10;

 # run a script to run a query
 run script '/home/vagrant/c/kafka-examples/ksql/user_profile_pretty.ksql';

# to see query details
describe extended user_profile_pretty;
# along with ksql-datagen see messages processed per second
# to see the query
select description from user_profile_pretty emit changes;

# terminate queries to be able to drop the stream
drop stream user_profile_pretty;
# will tell us the queries running against it
terminate query_id
eg
terminate  CSAS_USER_PROFILE_PRETTY_15;

rbeg
# what is a table - state at a point of time
# create topic COUNTRY-CSV
kafka-topics --zookeeper master.e4rlearning.com:2181 --create --topic CITY-CSV --partitions 1 --replication-factor 1

# WRITE INTO THE COUNTRY-CSV TOPIC with keys
kafka-console-producer --bootstrap-server master.e4rlearning.com:9092 --topic CITY-CSV --property parse.key=true --property key.separator=: << EOF
DL:Delhi
BL:Bengaluru
CN:Chennai
KO:Kota
MU:Mumbai
SO:Sonepat
PA:Panipat
EOF
# check that topic has data
kafka-console-consumer --bootstrap-server master.e4rlearning.com:9092 --topic CITY-CSV --from-beginning --property print.key=true

# create a table against the country-csv topic
CREATE TABLE CITYTABLE (citycode VARCHAR PRIMARY KEY, cityname VARCHAR) WITH (KAFKA_TOPIC='CITY-CSV', value_format='DELIMITED');

SET 'auto.offset.reset'='earliest';

select citycode, cityname from citytable emit changes;

joins
# the joined values are going to show up after we freshly populate stream data
# a stream left joining a table - join is got to be on the primary key
select up.*, ct.* from 
userprofilestream up 
left join citytable ct 
on ucase(up.citycode) = ct.citycode
emit changes
limit 5;

create stream up_ct_joined as 
select ct.citycode, up.firstname + ' ' + up.lastname + ' from '
+ ct.cityname + ' has a rating of ' +
cast(up.rating as varchar) + ' - ' +
case when rating < 2.5 then 'Poor' 
when rating between 2.5 and 4.2 then 'Good'
else 'Excellent' end 
as description 
from 
userprofilestream up 
left join citytable ct 
on up.citycode = ct.citycode;

select * from up_ct_joined emit changes;

##########################################################
push queries vs pull queries
###########################################################
# A push query is a form of query issued by a client that subscribes to a result as it changes in real-time. A good example of a push query is subscribing to a particular user's geographic location
# The query requests the map coordinates, and because it's a push query, any change to the location is "pushed" over a long-lived connection to the client as soon as it occurs. 
# This is useful for building programmatically controlled microservices, real-time apps, or any sort of asynchronous control flow
# The result of a push query isn't persisted to a backing Kafka topic. If you need to persist the result of a query to a Kafka topic, use a CREATE TABLE AS SELECT or CREATE STREAM AS SELECT statement

# A pull query is a form of query issued by a client that retrieves a result as of "now", like a query against a traditional RDBS.

# As a dual to the push query example, a pull query for a geographic location would ask for the current map coordinates of a particular user. 

# Because it's a pull query, it returns immediately with a finite result and closes its connection. 
# This is ideal for rendering a user interface once, at page load time. 
# It's generally a good fit for any sort of synchronous control flow

# Persistent queries are server-side queries that run indefinitely processing rows of events. You issue persistent queries by deriving new streams and new tables from existing streams or tables

###############################
### pull queries          #####
###############################

# pull query against aggregate or rowkey column must be included

# create a stream, topic will be created
CREATE STREAM driverLocations (driverId INTEGER KEY, citycode VARCHAR, city VARCHAR, driverName VARCHAR)
  WITH (kafka_topic='driverlocations',value_format='json', partitions=1);

create table cityDrivers as select citycode, count(*) as numDrivers from driverLocations group by citycode;

# insert data into the stream
INSERT INTO driverLocations (driverId, citycode, city, driverName) VALUES (1, 'MU', 'Mumbai', 'Mohan');
INSERT INTO driverLocations (driverId, citycode, city, driverName) VALUES (2, 'DL', 'Delhi', 'Shyam');
INSERT INTO driverLocations (driverId, citycode, city, driverName) VALUES (3, 'CN', 'Chennai', 'Sundar');
INSERT INTO driverLocations (driverId, citycode, city, driverName) VALUES (4, 'HY', 'Hyderabad', 'Ananth');

```
-- note: as a pull query we don't use "emit"
select citycode, numdrivers from cityDrivers where citycode='HY';

INSERT INTO driverLocations (driverId, citycode, city, driverName) VALUES (1, 'MU', 'Mumbai', 'Mohan');

#######################
against a topic if we produce wrong data, it will be quietly ignored
########################

# create a CSV topic
kafka-topics --zookeeper master.e4rlearning.com:2181 --create --topic stockcsv --partitions 1 --replication-factor 1
# create a stream linked to the topic
create stream stockcsvstream (date varchar, symbol varchar, close double, qty int) with (value_format='DELIMITED', kafka_topic='stockcsv');
# see the stream being populated;
select * from stockcsvstream emit changes;
# populate stockcsv with proper and bad data 

kafka-console-producer --bootstrap-server master.e4rlearning.com:9092 --topic stockcsv << EOF
2021-02-21,ACC,231.23,1276
2021-02-21,NHPC,123.24,5767
EOF

kafka-console-producer --bootstrap-server master.e4rlearning.com:9092 --topic stockcsv << EOF
2021-02-21,ACC,some value,some qty
2021-02-21,Britannia,235.45
2021-02-21,INFY,945.21,25483
EOF

# error messages in ksql.out in logs directory

################################################
## with avro, we will see the schema being enforced and errors being thrown
################################################
# create an avro topic and write a record to it
kafka-topics --zookeeper master.e4rlearning.com:2181 --create --partitions 1 --replication-factor 1 --topic COMPLAINTS_AVRO

kafka-avro-console-producer  --broker-list master.e4rlearning.com:9092 --topic COMPLAINTS_AVRO \
--property value.schema='
{
  "type": "record",
  "name": "myrecord",
  "fields": [
      {"name": "customer_name",  "type": "string" }
    , {"name": "complaint_type", "type": "string" }
    , {"name": "trip_cost", "type": "float" }
    , {"name": "new_customer", "type": "boolean", "default": false}
    , {"name": "digital_pay", "type": "boolean", "default": false}
  ]
}' << EOF
{"customer_name":"Rajnish", "complaint_type":"Late arrival", "trip_cost": 219.60, "new_customer": false, "digital_pay": true}
EOF

curl -s -X GET http://master.e4rlearning.com:8081/subjects/COMPLAINTS_AVRO-value/versions

curl -s -X GET http://master.e4rlearning.com:8081/subjects/COMPLAINTS_AVRO-value/versions/1 | jq '.'

# avro will detect data not consistent with scehma
kafka-avro-console-producer  --broker-list master.e4rlearning.com:9092 --topic COMPLAINTS_AVRO \
--property value.schema.id=1 << EOF
{"customer_name":"Bad Data", "complaint_type":"Bad driver", "trip_cost": 22.40, "new_customer": true, "digital_pay": false}
EOF

# create a new stream for the new schema
create stream complaints_avro_v1 with (kafka_topic='COMPLAINTS_AVRO', value_format='AVRO');

##########################################
#### avro schema evolution demo with ksql 
##########################################
{
    "name": "number_of_rides",
    "type": "int",
    "default": 1
}

kafka-avro-console-producer  --broker-list master.e4rlearning.com:9092 --topic COMPLAINTS_AVRO \
--property value.schema='
{
  "type": "record",
  "name": "myrecord",
  "fields": [
      {"name": "customer_name",  "type": "string" }
    , {"name": "complaint_type", "type": "string" }
    , {"name": "trip_cost", "type": "float" }
    , {"name": "new_customer", "type": "boolean"}
    , {"name": "number_of_rides", "type": "int", "default" : 1}
  ]
}' << EOF
{"customer_name":"Ed", "complaint_type":"Dirty car", "trip_cost": 129.10, "new_customer": true, "number_of_rides": 0}
EOF

# create a new stream for the new schema
create stream complaints_avro_v2 with (kafka_topic='COMPLAINTS_AVRO', value_format='AVRO');

# we have now a stream against two versions of the same schema and run a select to see schema evolution working
# run the avro console producers again to verify that we have full compatibility - we can write to the topic leaving with and without the number field and old cllients can read 

kafka-avro-console-producer  --broker-list master.e4rlearning.com:9092 --topic COMPLAINTS_AVRO \
--property value.schema.id=6 << EOF
{"customer_name":"Sajal Mukherjee", "complaint_type":"Good driver", "trip_cost": 12.40, "new_customer": true, "digital_pay": true}
EOF

kafka-avro-console-producer  --broker-list master.e4rlearning.com:9092 --topic COMPLAINTS_AVRO \
--property value.schema.id=7 << EOF
{"customer_name":"Neeta Kapoor", "complaint_type":"Bad Driver Doesn't Curse", "trip_cost": 112.31, "new_customer": false, "number_of_rides": 73}
EOF

#######################################################
###       create a nested stream                   ####
###       create a raw stream from nested stream   ####
###       key the raw stream                       ####

kafka-topics --zookeeper master.e4rlearning.com:2181 --create --partitions 1 --replication-factor 1 --topic WEATHERNESTED
# jsonl is json lines
cat /home/vagrant/c/kafka-examples/ksql/demo-weather.jsonl | kafka-console-producer --broker-list master.e4rlearning.com:9092 --topic WEATHERNESTED

# see the nested stream in ksql console

SET 'auto.offset.reset'='earliest';


CREATE STREAM weather 
      (city STRUCT <name VARCHAR, citycode VARCHAR, latitude DOUBLE, longitude DOUBLE>, 
       description VARCHAR, 
       clouds BIGINT, 
       deg BIGINT, 
       humidity BIGINT, 
       pressure DOUBLE, 
       rain DOUBLE) 
WITH (KAFKA_TOPIC='WEATHERNESTED', VALUE_FORMAT='JSON');

SELECT city ->name AS city_name, city->citycode AS city_code, city->latitude as latitude, city->longitude as longitude, description, rain from weather emit changes; 

create stream weatherraw with (value_format='AVRO') as SELECT city->name AS city_name, city->citycode AS city_code, city->latitude as latitude, city->longitude as longitude, description, rain from weather ;  
-- note AVRO

# we do not have a key for weatherraw
describe extended weatherraw;

create stream weatherrekeyed as select * from weatherraw partition by city_name;  

describe extended weatherrekeyed;
# note we have a key

create table weathernow(CITY_NAME VARCHAR PRIMARY KEY) with (kafka_topic='WEATHERREKEYED', value_format='AVRO');  

select * from weathernow emit changes;

select * from weathernow where city_name = 'Kota' emit changes;

# see the ktable changes being reflect
Let's make it rain! At UNIX prompt

cat /home/vagrant/c/kafka-examples/ksql/demo-weather-changes.jsonl | kafka-console-producer --broker-list master.e4rlearning.com:9092 --topic WEATHERNESTED  

select * from weathernow where city_name = 'Delhi' emit changes;

rb - rekeying a stream, table 
# Repartition

_When you use KSQL to join streaming data, you must ensure that your streams and tables are co-partitioned, which means that input records on both sides of the join have the same configuration settings for partitions.

kafka-topics --zookeeper master.e4rlearning.com:2181 --create --partitions 2 --replication-factor 1 --topic DRIVER_PROFILE

kafka-console-producer --bootstrap-server master.e4rlearning.com:9092 --topic DRIVER_PROFILE << EOF 
{"driver_name":"Mr Speedy", "citycode":"BL", "rating":2.4}
EOF  

CREATE STREAM DRIVER_PROFILE (driver_name VARCHAR, citycode VARCHAR, rating DOUBLE) 
  WITH (VALUE_FORMAT = 'JSON', KAFKA_TOPIC = 'DRIVER_PROFILE');

select dp.driver_name, ct.cityname, dp.rating 
from DRIVER_PROFILE dp 
left join CITYTABLE ct on ct.citycode=dp.citycode emit changes;    

Can't join `DP` with `CT` since the number of partitions don't match. `DP` partitions = 2; `CT` partitions = 1. Please repartition either one so that the number of partitions match.

## We can fix this by   co-partitioning, use the PARTITION BY clause. At KSQL prompt

create stream driverprofile_rekeyed with (partitions=1) as select * from DRIVER_PROFILE partition by driver_name;  

select dp2.driver_name, ct.cityname, dp2.rating 
from DRIVERPROFILE_REKEYED dp2 
left join CITYTABLE ct on ct.citycode=dp2.citycode emit changes;    

# feed in the data again for mr speedy's city to be picked up and reflect

```
## merging streams
## datagen into the two topics
ksql-datagen schema=/home/vagrant/c/kafka-examples/ksql/schemas/riderequest-north.avro format=avro topic=riderequest-north key=rideid iterations=10000 msgRate=1

ksql-datagen schema=/home/vagrant/c/kafka-examples/ksql/schemas/riderequest-south.avro format=avro topic=riderequest-south key=rideid maxInterval=5000 iterations=10000 msgRate=1

create stream rr_north_raw with (kafka_topic='riderequest-north', value_format='avro');  

create stream rr_south_raw with (kafka_topic='riderequest-south', value_format='avro');   

select * from rr_north_raw emit changes; 

select * from rr_south_raw emit changes;

create stream rr_country as select 'North' as data_source, * from rr_north_raw;  

insert into rr_country select 'South' as data_source, * from rr_south_raw;  

select * from rr_country emit changes; 

# Windows
- how many requests are arriving each time period
- At KSQL prompt
```
select data_source, city_name, count(*) 
from rr_country 
window tumbling (size 60 seconds) 
group by data_source, city_name emit changes;  
```

# see the users who in north or south in particular city asked for rides in the last minute
select data_source, city_name, COLLECT_LIST(user)
from rr_country
window tumbling (size 60 seconds) 
group by data_source, city_name emit changes;   

# see the users as per session windows
select data_source, city_name, COLLECT_LIST(user) 
from rr_country WINDOW SESSION (60 SECONDS) 
group by data_source, city_name emit changes;


select TIMESTAMPTOSTRING(WindowStart(), 'HH:mm:ss')
, TIMESTAMPTOSTRING(WindowEnd(), 'HH:mm:ss')
, data_source
, TOPK(city_name, 3)
, count(*)
FROM rr_country
WINDOW TUMBLING (SIZE 1 minute)
group by data_source
emit changes;
```

SELECT DATA_SOURCE, COUNT(*) FROM rr_country 
WINDOW HOPPING(SIZE 10 SECONDS, ADVANCE BY 5 SECONDS) 
GROUP BY DATA_SOURCE EMIT CHANGES;


# Geospacial
- create stream - distance of car to waiting rider
- At KSQL prompt

```

select * from rr_country emit changes;

describe rr_country;

create stream rr_country_rekeyed with (partitions = 1) as select * from rr_country;

create stream requested_journey as
select rr.latitude as from_latitude
, rr.longitude as from_longitude
, rr.user
, rr.city_name as city_name
, w.city_code
, w.latitude as to_latitude
, w.longitude as to_longitude
, w.description as weather_description
, w.rain 
from rr_country_rekeyed rr 
left join weathernow w on rr.city_name = w.city_name;   


create stream ridetodest as 
select user
, city_name
, city_code
, weather_description
, rain 
, GEO_DISTANCE(from_latitude, from_longitude, to_latitude, to_longitude, 'km') as dist
from requested_journey;  


```


```
select user + ' is travelling ' + cast(round(dist) as varchar) +' km to ' + city_name + ' where the weather is reported as ' + weather_description +  ' and has received rainfall = ' + cast(rain as varchar) + ' '
from ridetodest emit changes;  

Sundar is travelling 164 km to Hyderabad where the weather is reported as light rain and has received rainfall = 3.0                 
Manoj is travelling 175 km to Sonepat where the weather is reported as rainy and has received rainfall = 12.2                        
Ananth is travelling 204 km to Bengaluru where the weather is reported as fog and has received rainfall = 2.0                        
Niraj is travelling 815 km to Panipat where the weather is reported as rainy and has received rainfall = 12.0
```


create stream nsefo-stream 