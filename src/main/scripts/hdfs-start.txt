# environmental variable HADOOP_CONFI_DIR is set - verify
echo $HADOOP_CONF_DIR
#check the configurations
# default file system is set to hdfs
cat /home/vagrant/hadoop/etc/hadoop/core-site.xml
# check hdfs configurations
# dfs.replication set to 1, namenode, datanode, secondary namenode directories are set
# web ui is enabled
cat /home/vagrant/hadoop/etc/hadoop/hdfs-site.xml
# format the namenode
hdfs namenode -format
# start the daemons
# start the namenode
hdfs --daemon start namenode
# verify
jps
# start the data node
hdfs --daemon start datanode
# verify - in java processes should see namenode datanode
jps
# set up hdfs
# create home directory for user 
hdfs dfs -mkdir -p /user/vagrant
# create the temp directory and make it world writable
hdfs dfs -mkdir /tmp
hdfs dfs -chmod 777 /tmp
# create the spark application history that will come into use later
hdfs dfs -mkdir /spah
# make the spark applicaiton history world writable with the sticky bit turned on
hdfs dfs -chmod 1777 /spah
